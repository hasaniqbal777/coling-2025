---
title: Tutorials
layout: single
excerpt: "COLING 2025 Tutorials"
permalink: /program/tutorials/
sidebar: 
    nav: "program"
toc: true
toc_sticky: true
---

## Sunday, January 19, 2025

### Tutorial 1: Speculative Decoding for Efficient LLM Inference

**Organizers:** *Heming Xia, Cunxiao Du, Yongqi Li, Qian Liu and Wenjie Li*  
**Time:** 09:00 - 12:30  
**Location:** Capital Suite 7  
**Abstract:** This tutorial presents a comprehensive introduction to Speculative Decoding (SD), an advanced technique for LLM inference acceleration that has garnered significant research interest in recent years. SD
is introduced as an innovative decoding paradigm to mitigate the high inference latency stemming from
autoregressive decoding in LLMs. At each decoding step, SD efficiently drafts several future tokens and
then verifies them in parallel. This approach, unlike traditional autoregressive decoding, facilitates the
simultaneous decoding of multiple tokens per step, thereby achieving promising 2x-4x speedups in LLM
inference while maintaining original distributions. This tutorial delves into the latest techniques in SD,
including draft model architectures and verification strategies. Additionally, it explores the acceleration
potential and future research directions in this promising field. We aim for this tutorial to elucidate the current research landscape and offer insights for researchers interested in Speculative Decoding, ultimately
contributing to more efficient LLM inference.

### Tutorial 2: From Theory to Practice: A Hands-on Tutorial in Explainable NLP

**Organizers:** *Wafa Abdullah Alrajhi, Nourah Alangari, and Hend Al-Khalifa*  
**Time:** 09:00 - 12:30  
**Location:** Online Only  
**Abstract:** This tutorial aims to explore the intersection between XAI and the field of Natural Language Processing (NLP) by highlighting its significance for various tasks. By combining both theoretical and practical
aspects of XAI, we provide a comprehensive overview of its definitions and review the state-of-the-art
methodologies employed to explain NLP models. While previous tutorials have focused on establishing
the theoretical foundations of the XAI field and providing essential concepts to the audience, this tutorial
seeks to bridge the gap between theory and practice. We will discuss and demonstrate the most commonly
used XAI techniques, the prominent libraries applicable to different models, and the limitations of each
method. Furthermore, we will highlight the methods that can be utilized to visualize these models while
performing specific NLP tasks.

### Tutorial 3: Hands-On Tutorial: Labeling with LLM and Human-in-the-Loop

**Organizers:** *Ekaterina Artemova, Akim Tsvigun, Dominik Schlechtweg, Natalia Fedorova, Sergei Tilga, Boris Obmoroshev and Konstantin Chernyshev*  
**Time:** 14:00-17:30  
**Location:** Capital Suite 5  
**Abstract:** Training and deploying machine learning models relies on a large amount of human-annotated data. As human labeling becomes increasingly expensive and time-consuming, recent research has developed multiple strategies to speed up annotation and reduce costs and human workload: generating synthetic training
data, active learning, and hybrid labeling. This tutorial is oriented toward practical applications: we will
present the basics of each strategy, highlight their benefits and limitations, and discuss in detail real-life
case studies. Additionally, we will walk through best practices for managing human annotators and controlling the quality of the final dataset. The tutorial includes a hands-on workshop, where attendees will
be guided in implementing a hybrid annotation setup. This tutorial is designed for NLP practitioners
from both research and industry backgrounds who are involved in or interested in optimizing data labeling
projects.

### Tutorial 4: LLMs in Education: Novel Perspectives, Challenges, and Opportunities

**Organizers:** *Bashar Alhafni, Sowmya Vajjala, Stefano Banno, Kaushal Kumar Maurya and Ekaterina Kochmar*  
**Time:** 14:00 - 17:30  
**Location:** Capital Suite 7  
**Abstract:** The role of large language models (LLMs) in education is an increasing area of interest today, considering the new opportunities they offer for teaching, learning, and assessment. This cutting-edge tutorial provides an overview of the educational applications of NLP and the impact that the recent advances in LLMs have
had on this field. We will discuss the key challenges and opportunities presented by LLMs, grounding
them in the context of four major educational applications: reading, writing, and speaking skills, and intelligent tutoring systems (ITS). This tutorial is designed for researchers and practitioners interested in the educational applications of NLP and the role LLMs have to play in this area. It is the first of its kind to
address this timely topic.

### Tutorial 5: EduRAG: Crafting Clever Educational Chatbots and Advanced QA Systems with RetrievalAugmented Generation

**Organizers:** *Noorhan Abbas and Saad Ezzini*  
**Time:** 14:00 - 17:30  
**Location:** Online Only  
**Abstract:** This cutting-edge tutorial, aims to introduce and clarify Retrieval-Augmented Generation (RAG) for the international computational linguistics community. RAG combines retrieval-based and generation-based models to produce accurate and contextually relevant outputs, making it a powerful tool for developing intelligent educational tools such as chatbots and question answering systems.  
The tutorial will cover the following key topics:
- An introduction to RAG, its significance, and diverse applications, particularly in the educational sector.
- A detailed exploration of the core components and working mechanisms of RAG, highlighting the interaction between retrieval and generation components.
- An overview of different retrieval methods, including traditional and vector-based approaches, and the role of vector databases and embedding models in optimizing retrieval processes.
- Integration of Large Language Models (LLMs) within the RAG framework and the importance of prompt engineering in fine-tuning responses.
- Current challenges and future directions in the field of RAG.

To enhance practical understanding, two hands-on sessions will be included. The first session will focus on using the Unstructured API to pre-process various documents, such as PDF and PowerPoint files, creating metadata to make the retrieval process more efficient with LLMs. The second session will demonstrate different RAG pipelines and evaluation methods using the Haystack library, providing participants with hands-on experience in implementing and assessing RAG systems.  
The tutorial is designed for NLP researchers and practitioners, AI and machine learning engineers, educators, software developers, graduate students and academics. It will equip attendees with the knowledge and tools to implement RAG, fostering innovation and enhancing the capabilities of educational chatbots and question answering systems globally.
For more info, please refer to the tutorial website: [https://ezzini.github.io/edurag/](https://ezzini.github.io/edurag/)

## Monday, January 20, 2025

### Tutorial 6: Connecting Ideas in Lower-Resource Scenarios: NLP for National Varieties, Creoles, and Other Low-Resource Scenarios

**Organizers:** *Aditya Joshi, Diptesh Kanojia, Heather Lent, Hour Kaing and Haiyue Song*  
**Time:** 09:00 - 17:30  
**Location:** Conference Hall B (C)  
**Abstract:** Despite excellent results on benchmarks over a small subset of languages, large language models struggle to process text from languages situated in ‘lower-resource’ scenarios such as dialects/sociolects (national or social varieties of a language), Creoles (languages arising from linguistic contact between multiple languages) and other low-resource languages. This introductory tutorial will identify common challenges, approaches, and themes in natural language processing (NLP) research for confronting and overcoming
the obstacles inherent to data-poor contexts. By connecting past ideas to the present field, this tutorial aims
to ignite collaboration and cross-pollination between researchers working in these scenarios. Our notion
of ‘lower-resource’ broadly denotes the outstanding lack of data required for model training - and may be
applied to scenarios apart from the three covered in the tutorial.

### Tutorial 7: Safety Issues for Generative AI

**Organizers:**  *Haonan Li, Xudong Han, Emad A. Alghamdi, Shom Lin, Monojit Choudhury, Jingfeng Zhang, Paul Rottger and Timothy Baldwin*  
**Time:** 09:00 - 12:30  
**Location:** Capital Suite 7  
**Abstract:** This tutorial will provide an in-depth exploration of safety issues for generative AI models, covering a broad range of sub-topics, including a risk taxonomy, adversarial attack types, safety evaluation, defense mechanisms, red teaming for multi-modal models, and agentic AI. The goal is to brief attendees on the
latest advancements and emerging trends in the field, enabling them to identify and mitigate vulnerabilities
in generative AI systems. Participants will gain insights into recent research developments, open research
questions, and novel research directions. This cutting-edge tutorial is designed for AI researchers, developers, and security professionals with a basic knowledge of red teaming and AI safety.

### Tutorial 8: Hallucinative Foundation Models: Characterization, Quantification, Avoidance, and Mitigation

**Organizers:** *Vipula Rawte, Aman Chadha, Amit Sheth and Amitava Das*  
**Time:** 09:00 - 12:30  
**Location:** Online Zoom  
**Abstract:** In the fast-paced domain of Large Language Models (LLMs), the issue of hallucination is a prominent challenge. Despite continuous endeavors to address this concern, it remains a highly active area of research within the LLM landscape. Grasping the intricacies of this problem can be daunting, especially
for those new to the field. This tutorial aims to bridge this knowledge gap by introducing the emerging
realm of hallucination in LLMs. It will comprehensively explore the key aspects of hallucination, including benchmarking, detection, and mitigation techniques. Furthermore, we will delve into the specific constraints and shortcomings of current approaches, providing valuable insights to guide future research
efforts for participants.

### Tutorial 9: Bridging Linguistic Theory and AI: Usage-Based Learning in Humans and Machines

**Organizers:** *Claire Bonial, Harish Tayyar Madabushi, Nikhil Krishnaswamy and James Pustejovsky*  
**Time:** 14:00 - 17:30  
**Location:** Capital Suite 7  
**Abstract:** Usage-based theories of human language, such as Construction Grammar, have been compelling theoretical lenses through which to view and evaluate what LLMs know and understand of language because of
the parallels between usage-based learning and the data-driven “learning” of pre-trained models. However,
a key difference between a usage-based learning account for humans and that of LLMs is in embodiment
and multimodality—for the most part, LLMs use text alone, whereas usage-based theories posit that each
token of linguistic experience is stored with a wealth of experiential information that enriches the symbol through cross-modal association. Therefore, the first goal of this tutorial is to provide a summary of
language acquisition and second language learning from a usage-based theoretical linguistic perspective.
With this understanding of human usage-based learning, we will turn to evidence demonstrating the ways
in which machine learning, primarily via large, pre-trained vision and language models, does and does
not parallel human learning. The overarching goal of this is not to say that the two processes are similar
or dissimilar in order to conclude that dissimilarity denotes inferiority (if the knowledge arrived at is the
same, then it may not matter how it was learned). Rather, we explore the resulting differences in what is
known and understood about the world, and take this as a starting point for considering how to supplement
and improve natural language understanding (NLU), particularly for physically situated applications. Our
target audience is those interested in the intersection of linguistic theory and NLU implementations, such
as human-robot interaction.
